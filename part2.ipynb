{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "persist_directory = 'docs/chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7t/ys9t4mvn1fx74wckbywxd2sw0000gp/T/ipykernel_15979/2897838732.py:1: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings()\n",
      "/var/folders/7t/ys9t4mvn1fx74wckbywxd2sw0000gp/T/ipykernel_15979/2897838732.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(\n"
     ]
    }
   ],
   "source": [
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is logistic regression?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_mmr = vectordb.max_marginal_relevance_search(question,k=2, fetch_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'author': 'Sepp Hochreiter', 'creationdate': '2024-01-19T16:06:01+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-01-19T16:06:01+00:00', 'page': 140, 'page_label': '129', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'Script_Machine Learning Basic Techniques.pdf', 'subject': '', 'title': 'Basic Methods of Data Analysis', 'total_pages': 175, 'trapped': '/False'}, page_content='7.4. Binary Models 129\\nIf x is in the k-th class then y = (0 , . . . ,0, 1, 0, . . . ,0), where the “1” is at position k in the\\nvector y.\\nThe likelihood of iid data is\\nL({z}; w) = p({z}; w) =\\nNY\\nn=1\\nKY\\nk=1\\np(yn = ek | xn; w)[yn]k p(xn) = (7.34)\\nNY\\nn=1\\np(xn)\\nKY\\nk=1\\np(yn = ek | xn; w)[yn]k\\nbecause\\nKY\\nk=1\\np(yn = ek | xn; w)[yn]k = p(yn = er | xn; w) for yn = er . (7.35)\\nThe log-likelihood is\\nln L({z}; w) =\\nKX\\nk=1\\nNX\\nn=1\\n[yn]k ln p(yn = ek | xn; w) +\\nNX\\nn=1\\nln p(xn) . (7.36)\\nTherefore the criterion\\nKX\\nk=1\\nNX\\nn=1\\n[yn]k ln p(yn = ek | xn; w) (7.37)\\nis a natural loss function which is called cross entropy. Note that [yn]k is the observed probability\\np(yn = ek) which is one if yn = ek and zero otherwise.\\nTherefore above formula is indeed the cross entropy for discrete distributions.\\n7.4.2 Logistic Regression\\nA function g mapping x onto R can be transformed into a probability by the sigmoidal function\\n1\\n1 + e− g(x;w) (7.38)\\nwhich is depicted in Fig. 7.2.\\nNote that\\n1 − 1\\n1 + e− g(x;w) = e− g(x;w)\\n1 + e− g(x;w) . (7.39)\\nWe set\\np(y = 1 | x; w) = 1\\n1 + e− g(x;w) (7.40)'),\n",
       " Document(metadata={'author': 'Sepp Hochreiter', 'creationdate': '2024-01-19T16:06:01+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-01-19T16:06:01+00:00', 'page': 94, 'page_label': '83', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'Script_Machine Learning Basic Techniques.pdf', 'subject': '', 'title': 'Basic Methods of Data Analysis', 'total_pages': 175, 'trapped': '/False'}, page_content='4.2. Introductory Example: Selecting a Regression Model 83\\nTo determine λ is a task for model selection, more precise a task of hyperparameter selection.\\nThe regularization parameter λ is a hyperparameter of our ridge regression method. The best\\nhyperparameter may be found by using avalidation set, which is a hold-out set of samples besides\\nthe training set. The validation set is not used for adjusting the parametersw but only to adjust the\\nhyperparameter λ. We train with different λs on the training set and check for the λ with lowest\\nvalidation risk. The λ with the lowest risk is the one we choose.'),\n",
       " Document(metadata={'author': 'Sepp Hochreiter', 'creationdate': '2024-01-19T16:06:01+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-01-19T16:06:01+00:00', 'page': 83, 'page_label': '72', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'Script_Machine Learning Basic Techniques.pdf', 'subject': '', 'title': 'Basic Methods of Data Analysis', 'total_pages': 175, 'trapped': '/False'}, page_content='We already addressed the question of cost. That is how expensive is a certain error. A related\\nissue is the kind of noise on the measurements and on the class labels produced in our example\\nby humans. Perhaps the fishes on the wrong side of the boundary in Fig. 4.7 are just error of the\\nhuman experts. Another possibility is that the picture did not allow to extract the correct lightness\\nvalue. Finally, outliers in lightness or width as in Fig. 4.7 may be typically for salmons and sea\\nbass.\\n4.2 Introductory Example: Selecting a Regression Model\\nThis example is the polynomial curve fitting example from Section 1.1 in Bishop [2006].\\nWe observe a real-valued input variable x from which we want to predict the target variable\\ny. The samples stem from the function y(x) = sin(2 πx) plus noise. We are given a training\\nset comprising N observations of x (the independent variable) denoted by x = (x1, . . . , xN )T .\\nWe use subscripts for samples in this case, since below we use polynomials. Sample indices are\\notherwise superscript. The corresponding observations of the values ofy (the dependent variable)\\nare denoted y = (y1, . . . , yN )T . Figure 4.8 shows a plot of a training set comprising N = 10\\nsamples. The input data set x in Figure 4.8 was generated by uniformly spaced values in the\\nrange [0, 1]. The target values y are obtained by first computing sin(2πx) and then adding a small\\nGaussian noise with zero mean. Therefore\\nyn = sin(2 π xn) + ϵn , (4.1)')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7.4. Binary Models 129\\nIf x is in the k-th class then y = (0 , . . . ,0, 1, 0, . . . ,0), where the '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mmr[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7t/ys9t4mvn1fx74wckbywxd2sw0000gp/T/ipykernel_15979/16750302.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model='gpt-4o', temperature=0)\n"
     ]
    }
   ],
   "source": [
    "document_content_description = \"Machine Learning Basic Techniques\"\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "# retriever = SelfQueryRetriever.from_llm(\n",
    "#     llm,\n",
    "#     vectordb,\n",
    "#     document_content_description,\n",
    "#     verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs = retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"o3-mini\")\n",
    "compressor = LLMChainExtractor.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7t/ys9t4mvn1fx74wckbywxd2sw0000gp/T/ipykernel_15979/1955647316.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  compressed_docs = compression_retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'author': 'Sepp Hochreiter', 'creationdate': '2024-01-19T16:06:01+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-01-19T16:06:01+00:00', 'page': 140, 'page_label': '129', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'Script_Machine Learning Basic Techniques.pdf', 'subject': '', 'title': 'Basic Methods of Data Analysis', 'total_pages': 175, 'trapped': '/False'}, page_content='A function g mapping x onto R can be transformed into a probability by the sigmoidal function 1 1 + e− g(x;w) (7.38) which is depicted in Fig. 7.2. Note that 1 − 1 1 + e− g(x;w) = e− g(x;w) 1 + e− g(x;w) . (7.39) We set p(y = 1 | x; w) = 1 1 + e− g(x;w) (7.40)')]\n",
      "Document 1:\n",
      "\n",
      "A function g mapping x onto R can be transformed into a probability by the sigmoidal function 1 1 + e− g(x;w) (7.38) which is depicted in Fig. 7.2. Note that 1 − 1 1 + e− g(x;w) = e− g(x;w) 1 + e− g(x;w) . (7.39) We set p(y = 1 | x; w) = 1 1 + e− g(x;w) (7.40)\n"
     ]
    }
   ],
   "source": [
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "print (compressed_docs)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
